<!DOCTYPE HTML>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R8H6D3D4TW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-R8H6D3D4TW');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mariya Hendriksen</title>

    <meta name="author" content="Mariya Hendriksen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Mariya Hendriksen
                </p>
                <p>I am intern on the <a href="https://www.microsoft.com/en-us/research/group/game-intelligence/">Game Intelligence</a> team at Microsoft Research Cambridge.
                  I completed my PhD at the University of Amsterdam where I worked on <a href="data/thesis.pdf">multimodal machine learning for information retrieval</a> under the supervision of <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> and <a href="https://pgroth.com/">Paul Groth</a>.
                </p>
                <p>
                  I hold a Master’s degree in Artificial Intelligence from KU Leuven and a Bachelor's degree in Computational Linguistics from Novosibirsk State University.
                  Throughout my academic journey, I've interned at several AI labs, including the <a href="https://gemini.google.com/">Gemini</a> team at Google, Bloomberg AI, Amazon Alexa,
                  <a href="https://liir.cs.kuleuven.be/">LIIR</a> at KU Leuven, and ETH Zurich.
                </p>
                <p>
                  I was born in Ukraine and grew up in the Far North of Siberia, Russia. My multicultural background shaped a commitment to fostering diverse and inclusive research environments.
                  As such, I serve as the General Chair for the <a href="https://wimlworkshop.org/">Women in Machine Learning (WiML)</a> at <a href="https://icml.cc/">ICML 2025</a>, and as a metor within the <a href="https://ivi.fnwi.uva.nl/ellis/inclusive-ai/">Inclusive AI</a> initiative.
                <p> Feel free to reach out for research discussions, collaboration ideas, or just to connect. </p>

                <p style="text-align:center">
                  <a href="mailto:mariya.hendriksen@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=QYN_2acAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mariyahendriksen">GitHub</a> &nbsp;/&nbsp;
                  <a href="https://bsky.app/profile/mariyahendriksen.bsky.social">Bsky</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/mariehendriksen">X</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/MariyaHendriksen.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/MariyaHendriksen.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>News</h2>
            <!-- Research Papers Section -->
            <h3>Publications</h3>
            <ul>
              <li>
                <span>(<span style="color: teal;">Apr 2025</span>) Paper 'Benchmark Granularity and Model Robustness for Image-Text Retrieval' accepted at SIGIR 2025.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Jul 2024</span>) Paper '<a href="https://arxiv.org/abs/2402.17510">Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning</a>' accepted at TMLR.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Dec 2023</span>) Paper '<a href="https://arxiv.org/abs/2402.17535">Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control</a>' accepted at ECIR 2024.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Dec 2022</span>) Our paper '<a href="https://arxiv.org/abs/2301.05174">Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study</a>' accepted at ECIR 2023.</span>
              </li>
<!--               <li>
                <span>(<span style="color: teal;">Nov 2023</span>) Published the paper '<a href="https://arxiv.org/abs/2402.07736">Multimodal Learned Sparse Retrieval for Image Suggestion</a>' as part of our team's participation in TREC 2023 AToMiC.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Dec 2022</span>) Our paper '<a href="https://arxiv.org/abs/2301.05174">Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study</a>' accepted at ECIR 2023.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Jul 2022</span>) Published the paper '<a href="https://arxiv.org/abs/2207.10355">Unimodal vs. Multimodal Siamese Networks for Outfit Completion</a>' as part of the Fashion Outfits Challenge at SIGIR 2022 Workshop on eCommerce.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Dec 2021</span>) The work '<a href="data/DC_note.pdf">Multimodal Retrieval in E-commerce: from Categories to Images, Text, and Back</a>' got accepted at the ECIR 2022 Doctoral Consortium.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Nov 2021</span>) Our paper about <a href="https://arxiv.org/abs/2112.11294">CLIP for zero-shot image retrieval</a> was accepted to the main track at ECIR 2022.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Jun 2020</span>) Our paper '<a href="https://arxiv.org/abs/2012.08777">Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs. Identified Customers</a>' accepted at SIGIR 2020 Workshop on eCommerce.</span>
              </li> -->
            </ul>
            <!-- Other Updates Section -->
            <h3>Milestones & Activities</h3>
            <ul>
              <li>
                <span>(<span style="color: teal;">Apr 2025</span>) Serving as the General Chair for the Women in Machine Learning symposium at ICML 2025.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Sep 2024</span>) Started a research internship at Microsoft Research Cambridge on the Game Intelligence team.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Apr 2024</span>) Started a research internship at Google Zurich on the Gemini team.</span>
              </li>
<!--               <li>
                <span>(<span style="color: teal;">Mar 2024</span>) Attended the ELLIS Winter School on Foundation Models.</span>
              </li> -->
              <li>
                <span>(<span style="color: teal;">Jul 2023</span>) Started a research internship at Bloomberg AI in London.</span>
              </li>
<!--               <li>
                <span>(<span style="color: teal;">Nov 2022</span>) Started a research internship at Alexa, Amazon Science in London.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Jun 2022</span>) Gave a talk on ‘Extending CLIP for Category-to-Image Retrieval in E-commerce’ at Amazon Luxembourg.</span>
              </li>
              <li>
                <span>(<span style="color: teal;">Apr 2020</span>) Attended MLSS 2020.</span>
              </li> -->
            </ul>
          </td>
        </tr>
        </tbody></tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/information-venn-diagram-mini.jpg" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17510">
                        <papertitle>Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning.</papertitle>
                      </a>
                      <br>
                      Maurits Bleeker*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> (co-first author)
                      <br>
                      <em>TMLR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17510">arXiv</a> /
                      <a href="data/bleekerhendriksen2024demonstrating.txt">bibtex</a> /
                      <a href="https://github.com/MauritsBleeker/svl-framework">Github</a>
                      <p></p>
                       We propose a framework to examine the shortcut learning problem in the context of Vision-Language contrastive representation learning with multiple captions per image. We show how this problem can be partially mitigated using a form of text reconstruction and implicit feature modification.
                      </p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir24.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17535">
                        <papertitle>Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control.</papertitle>
                      </a>
                      <br>
                      Thong Nguyen*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> (co-first author)
                      <br>
                      <em>ECIR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17535">arXiv</a> /
                      <a href="https://github.com/thongnt99/lsr-multimodal">Github</a>
                      <p></p>
                       We propose a framework for multimodal learned sparse retrieval.
                      </p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir23.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2301.05174">
                        <papertitle>Scene-Centric vs. Object-Centric Image-Text Cross-Modal Retrieval: A Reproducibility Study.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      Ernst Kuiper,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2023
                      <br>
                      <a href="https://arxiv.org/abs/2301.05174">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir23-object-centric-vs-scene-centric-CMR">Github</a>
                      <p></p>
                       
                      </p>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir22.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2112.11294">
                        <papertitle>Extending CLIP for Category-to-Image Retrieval in E-commerce<a href="data/paper_17.pdf">.</a></papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Maurits Bleeker,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      <a href="https://nanne.github.io/">Nanne van Noord</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2022
                      <br>
                      <a href="https://arxiv.org/abs/2112.11294">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir2022_category_to_image_retrieval">Github</a>
                      <p></p>

                      </p>
                    </td>
                  </tr>


                  <tr>
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/sigir-ecom21.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2012.08777">
                        <papertitle>Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs. Identified Customers<a href="data/thesis_benedict.pdf">.</a></papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Ernst Kuiper,
                      Pim Nauts,
                      Sebastian Schelter,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>SIGIR eCom</em>, 2020
                      <br>
                      <a href="https://arxiv.org/abs/2012.08777">arXiv</a>
                      <p></p>
                       
                      </p>
                    </td>
                  </tr>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Thanks to
                  <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>
                  for the template :)
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>

      </tr>
    </table>
  </body>
</html>
