<!DOCTYPE HTML>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R8H6D3D4TW"></script>
    <script data-goatcounter="https://mhendriksen.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-R8H6D3D4TW');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mariya Hendriksen</title>

    <meta name="author" content="Mariya Hendriksen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  <body>

  <!-- Static Header -->
  <div class="navbar-container" style="background-color: var(--bg-color); border-bottom: 1px solid var(--border-color); padding: 10px 0; margin-bottom: 0px;">
    <div style="max-width: 800px; margin: 0 auto; display: flex; justify-content: space-between; align-items: center; padding: 0 10px;">
      
      <!-- Name on the left -->
      <a href="#" style="font-family: var(--font-primary); font-size: 36px; font-weight: 600; color: var(--text-color); text-decoration: none; border: none;">
        Mariya Hendriksen
      </a>
      
      <!-- Navigation links on the right (TBD) -->
      <!-- <ul style="list-style: none; margin: 0; padding: 0; display: flex; gap: 25px;">
        <li><a href="#news" style="text-decoration: none; border: none;">News</a></li>
        <li><a href="#research" style="text-decoration: none; border: none;">Research</a></li>
      </ul> -->
    
  </div>
</div>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr style="padding:0px">
        <td style="padding:0px">

          <!-- Profile Section -->
          <div class="profile-content" style="max-width: 800px; margin: 0 auto; padding: 20px;">
            
            <!-- Photo -->
            <img src="images/MariyaHendriksen.jpeg" 
                 alt="profile photo" 
                 style="float: right; 
                        width: 200px; 
                        height: 200px; 
                        object-fit: cover; 
                        border-radius: 50%; 
                        margin: 0 0 20px 30px;">
                        
            <p>
              I am a Research Fellow at the University of Oxford, working with
              <strong><a href="https://www.jesus.ox.ac.uk/about-jesus-college/our-community/people/dr-oiwi-parker-jones/">ʻŌiwi Parker Jones</a></strong>
              and
              <strong><a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a></strong>. I am also a member of the <a href="https://ellis.eu/">ELLIS</a> Society.
              I am passionate about advancing multimodal machine learning by creating models that are adaptable, trustworthy, and informed by cognition.
            </p>
            
            <p>
              I earned my PhD in Artificial Intelligence at the University of Amsterdam, advised by
              <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
              and
              <a href="https://pgroth.com/">Paul Groth</a>.
              Before that, I obtained a MSc in AI from KU Leuven where advised by <a href="https://scholar.google.com/citations?user=O9hYMUUAAAAJ&hl=en">Marie-Francine Moens</a>.
            </p>
            
            <p>
              During my academic journey, I spent time at Microsoft Research, the
              <a href="https://gemini.google.com/">Gemini</a> team, Bloomberg AI, Amazon Alexa,
              <a href="https://liir.cs.kuleuven.be/">LIIR</a> at KU Leuven, and ETH Zurich as an intern.
            </p>
            
            <p>
              Alongside my research, I am committed to fostering diverse and inclusive research communities.
              As such, I organize the <a href="https://sites.google.com/wimlworkshop.org/wimlmentorship202526/">WiML Mentorship Program</a>,
              served as the General Chair for the <a href="https://wimlworkshop.org/">WiML</a> at <a href="https://icml.cc/">ICML 2025</a>,
              and mentored through the <a href="https://ivi.fnwi.uva.nl/ellis/inclusive-ai/">Inclusive AI</a> initiative.
            </p>
            
            <!-- Clear float before links -->
            <div style="clear: both;"></div>
            
            <p style="text-align:center; margin-top: 20px;">
              <a href="mailto:removethisifyourehuman-mariya.hendriksen@eng.ox.ac.uk">Email</a> &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?user=QYN_2acAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
              <a href="https://github.com/mariyahendriksen">GitHub</a> &nbsp;/&nbsp;
              <a href="https://bsky.app/profile/mariyahendriksen.bsky.social">Bsky</a> &nbsp;/&nbsp;
              <a href="https://twitter.com/mariehendriksen">X</a>
            </p>
            
          </div>

          <!-- News Section --> 
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-50px;"><tbody>
          <tbody>
          <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <h2 class="section-title">Updates</h2>

              <!-- Publications Section -->
              <h3 class="section-title">Publications</h3>
              <ul>
                <li class="news-item">
                  <span>(<span style="color: teal;">Nov 2025</span>) Presenting '<a href="data/hendriksen2025adapting.pdf">Adapting Vision-Language Models for Evaluating World Models</a>' at NeurIPS LAW 2025 as an <b>Oral</b>.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2025</span>) Presenting '<a href="data/hendriksen2025adapting.pdf">Human-VLM Collaboration for Evaluating World Models</a>' at CMU as a  <b>Spotlight</b>.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2025</span>) Presenting '<a href="data/recsys-genaiecom-2025.pdf">Image-Seeking Intent Prediction for Cross-Device Product Search</a>' at RecSys Gen AI for E-commerce 2025 as an <b>Oral</b>.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2025</span>) Paper '<a href="https://arxiv.org/abs/2407.15239">Benchmark Granularity and Model Robustness for Image-Text Retrieval</a>' accepted at SIGIR 2025.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Jul 2024</span>) Paper '<a href="https://arxiv.org/abs/2402.17510">Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning</a>' accepted at TMLR.</span>
                </li>
<!--                 <li class="news-item">
                  <span>(<span style="color: teal;">Dec 2023</span>) Paper '<a href="https://arxiv.org/abs/2402.17535">Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control</a>' accepted at ECIR 2024.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Dec 2022</span>) Our paper '<a href="https://arxiv.org/abs/2301.05174">Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study</a>' accepted at ECIR 2023.</span> -->
                </li>
              </ul>

              <!-- Milestones Section -->
              <h3 class="section-title">Milestones</h3>
              <ul>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2025</span>) Started as a Research Fellow at the University of Oxford, working with Philip Torr and ʻŌiwi Parker Jones.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2025</span>) Serving as the General Chair for the Women in Machine Learning symposium at ICML 2025.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2024</span>) Started a research internship at Microsoft Research on the Game Intelligence team.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2024</span>) Started a research internship with the Gemini team.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Jul 2023</span>) Started a research internship at Bloomberg AI with the Question Answering team.</span>
                </li>
              </ul>
            </td>
          </tr>
          </tbody></tbody></table>

          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-30px;"><tbody>
              <tr>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <h2 class="section-title">Research</h2>
              </td>
            </tr>
          </tbody></table>

          <!-- Research Papers -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                  <!-- Paper 1 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/neurips2025.jpg" alt="BarPlotsUniverse" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/pdf/2506.17967">
                        <papertitle>Adapting Vision-Language Models for Evaluating World Models.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Tabish Rashid,
                      David Bignell,
                      Raluca Georgescu,
                      Abdelhak Lemkhenter,
                      Katja Hoffman,
                      Sam Devlin*,
                      Sarah Parisot*
                      <br>
                      <em>Oral at NeurIPS LAW 2025</em>; Under submission
                      <br>
                      <a href="https://arxiv.org/pdf/2506.17967">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/vlms-for-wms">GitHub</a>
                      <p></p>
                       We focus on the challenge of automated evaluation for world model rollouts and introduce a structured semantic protocol for this domain. We also propose UNIVERSE, a method for efficient adaptation of VLMs to this scenario.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 2 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/information-venn-diagram-mini.jpg" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17510">
                        <papertitle>Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning.</papertitle>
                      </a>
                      <br>
                      Maurits Bleeker*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>TMLR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17510">arXiv</a> /
                      <a href="data/bleekerhendriksen2024demonstrating.txt">bibtex</a> /
                      <a href="https://github.com/MauritsBleeker/svl-framework">GitHub</a>
                      <p></p>
                       We propose a framework to examine the shortcut learning problem in the context of Vision-Language contrastive representation learning with multiple captions per image. We show how this problem can be partially mitigated using a form of text reconstruction and implicit feature modification.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 3 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir24.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17535">
                        <papertitle>Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control.</papertitle>
                      </a>
                      <br>
                      Thong Nguyen*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17535">arXiv</a> /
                      <a href="https://github.com/thongnt99/lsr-multimodal">GitHub</a>
                      <p></p>
                       We propose a framework for multimodal learned sparse retrieval.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 4 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir23.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2301.05174">
                        <papertitle>Scene-Centric vs. Object-Centric Image-Text Cross-Modal Retrieval: A Reproducibility Study.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      Ernst Kuiper,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2023
                      <br>
                      <a href="https://arxiv.org/abs/2301.05174">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir23-object-centric-vs-scene-centric-CMR">GitHub</a>
                      <p></p>

                      </p>
                    </td>
                  </tr>

                  <!-- Paper 5 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir22.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2112.11294">
                        <papertitle>Extending CLIP for Category-to-Image Retrieval in E-commerce<a href="data/paper_17.pdf">.</a></papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Maurits Bleeker,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      <a href="https://nanne.github.io/">Nanne van Noord</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2022
                      <br>
                      <a href="https://arxiv.org/abs/2112.11294">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir2022_category_to_image_retrieval">GitHub</a>
                      <p></p>

                      </p>
                    </td>
                  </tr>

                  <!-- Paper 6 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/sigir-ecom21.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2012.08777">
                        <papertitle>Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs. Identified Customers.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Ernst Kuiper,
                      Pim Nauts,
                      Sebastian Schelter,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>SIGIR eCom</em>, 2020
                      <br>
                      <a href="https://arxiv.org/abs/2012.08777">arXiv</a> / <a href="data/Svyatov_4_25.pdf">related work</a>
                      <p></p>

                      </p>
                    </td>
                  </tr>

          </tbody></table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              </tr>
            </tbody></table>
          </td>

        </tr>
      </table>

  </body>
</html>
