<!DOCTYPE HTML>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-R8H6D3D4TW"></script>
    <script data-goatcounter="https://mhendriksen.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-R8H6D3D4TW');
    </script>

    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Mariya Hendriksen</title>

    <meta name="author" content="Mariya Hendriksen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
    <!-- Initial page load animation -->
    <style>
    body {
      opacity: 0;
      transform: translateY(20px);
      transition: all 0.6s ease-out;
    }
    </style>
  </head>

  <body>
  <!-- Light/Dark mode switching -->
  <button id="theme-toggle" aria-label="Toggle dark / light mode">🌙</button>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <!-- Profile Section with Animation Class -->
          <div class="profile-content">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Mariya Hendriksen
                  </p>
                  <p>
                  I am a Research Fellow at the University of Oxford, working with 
                  <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a> 
                  and 
                  <a href="https://www.jesus.ox.ac.uk/about-jesus-college/our-community/people/dr-oiwi-parker-jones/">ʻŌiwi Parker Jones</a>.
                  My work focuses on multimodal machine learning for applications in neuroscience.
                  </p>
                  <p>
                    I completed my PhD in Artificial Intelligence at the University of Amsterdam, advised by 
                    <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> 
                    and 
                    <a href="https://pgroth.com/">Paul Groth</a>. 
                    I also hold an MSc in AI from KU Leuven and a BSc in Computational Linguistics from Novosibirsk State University. 
                  </p>
                  <p>
                    During my academic journey, I interned at Microsoft Research (Cambridge), the 
                    <a href="https://gemini.google.com/">Google Gemini</a> team, Bloomberg AI, Amazon Alexa, 
                    <a href="https://liir.cs.kuleuven.be/">LIIR</a> at KU Leuven, and ETH Zurich. 
                  </p>
                  <p>
                    Alongside my research, I am committed to fostering diverse and inclusive research communities.
                    As such I organize the <a href="https://sites.google.com/wimlworkshop.org/wimlmentorship202526/">WiML Mentorship Program</a>,
                    served as the General Chair for the <a href="https://wimlworkshop.org/">WiML</a> at <a href="https://icml.cc/">ICML 2025</a>,
                    and mentored through the <a href="https://ivi.fnwi.uva.nl/ellis/inclusive-ai/">Inclusive AI</a> initiative.
                  </p>

                  <p style="text-align:center">
                    <a href="mailto:mariya.hendriksen@eng.ox.ac.uk">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=QYN_2acAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/mariyahendriksen">GitHub</a> &nbsp;/&nbsp;
                    <a href="https://bsky.app/profile/mariyahendriksen.bsky.social">Bsky</a> &nbsp;/&nbsp;
                    <a href="https://twitter.com/mariehendriksen">X</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/MariyaHendriksen.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/MariyaHendriksen.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody></table>
          </div>

          <!-- News Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2 class="section-title">News</h2>
              
              <!-- Publications Section -->
              <h3 class="section-title">Publications</h3>
              <ul>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2025</span>) Paper 'Benchmark Granularity and Model Robustness for Image-Text Retrieval' accepted at SIGIR 2025.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Jul 2024</span>) Paper '<a href="https://arxiv.org/abs/2402.17510">Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning</a>' accepted at TMLR.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Dec 2023</span>) Paper '<a href="https://arxiv.org/abs/2402.17535">Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control</a>' accepted at ECIR 2024.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Dec 2022</span>) Our paper '<a href="https://arxiv.org/abs/2301.05174">Scene-centric vs. Object-centric Image-Text Cross-modal Retrieval: A Reproducibility Study</a>' accepted at ECIR 2023.</span>
                </li>
              </ul>
              
              <!-- Milestones Section -->
              <h3 class="section-title">Milestones & Activities</h3>
              <ul>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2025</span>) Started as a Research Fellow at the University of Oxford, working with Philip Torr and ʻŌiwi Parker Jones.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2025</span>) Serving as the General Chair for the Women in Machine Learning symposium at ICML 2025.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Sep 2024</span>) Started a research internship at Microsoft Research Cambridge on the Game Intelligence team.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Apr 2024</span>) Started a research internship at Google Zurich on the Gemini team.</span>
                </li>
                <li class="news-item">
                  <span>(<span style="color: teal;">Jul 2023</span>) Started a research internship at Bloomberg AI in London on the Question Answering team.</span>
                </li>
              </ul>
            </td>
          </tr>
          </tbody></tbody></table>

          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2 class="section-title">Research</h2>
              </td>
            </tr>
          </tbody></table>

          <!-- Research Papers -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                  <!-- Paper 1 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/neurips2025.jpg" alt="BarPlotsUniverse" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="data/hendriksen2025adapting.pdf">
                        <papertitle>Adapting Vision-Language Models for Evaluating World Models.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Tabish Rashid,
                      David Bignell,
                      Raluca Georgescu,
                      Abdelhak Lemkhenter,
                      Katja Hoffman,
                      Sam Devlin*,
                      Sarah Parisot*
                      <br>
                      <em>Under Submission</em>, 2025
                      <br>
                      <p></p>
                       We address the challenge of automated evaluation for world model rollouts by introducing a structured protocol and UNIVERSE, a method for adapting vision-language models through unified fine-tuning to assess temporal and semantic fidelity.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 2 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/information-venn-diagram-mini.jpg" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17510">
                        <papertitle>Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning.</papertitle>
                      </a>
                      <br>
                      Maurits Bleeker*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> (co-first author)
                      <br>
                      <em>TMLR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17510">arXiv</a> /
                      <a href="data/bleekerhendriksen2024demonstrating.txt">bibtex</a> /
                      <a href="https://github.com/MauritsBleeker/svl-framework">Github</a>
                      <p></p>
                       We propose a framework to examine the shortcut learning problem in the context of Vision-Language contrastive representation learning with multiple captions per image. We show how this problem can be partially mitigated using a form of text reconstruction and implicit feature modification.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 3 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir24.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2402.17535">
                        <papertitle>Multimodal Learned Sparse Retrieval with Probabilistic Expansion Control.</papertitle>
                      </a>
                      <br>
                      Thong Nguyen*,
                      <strong>Mariya Hendriksen*</strong>,
                      <a href="https://andrewyates.net/">Andrew Yates</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a> (co-first author)
                      <br>
                      <em>ECIR</em>, 2024
                      <br>
                      <a href="https://arxiv.org/abs/2402.17535">arXiv</a> /
                      <a href="https://github.com/thongnt99/lsr-multimodal">Github</a>
                      <p></p>
                       We propose a framework for multimodal learned sparse retrieval.
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 4 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir23.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2301.05174">
                        <papertitle>Scene-Centric vs. Object-Centric Image-Text Cross-Modal Retrieval: A Reproducibility Study.</papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      Ernst Kuiper,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2023
                      <br>
                      <a href="https://arxiv.org/abs/2301.05174">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir23-object-centric-vs-scene-centric-CMR">Github</a>
                      <p></p>
                       
                      </p>
                    </td>
                  </tr>

                  <!-- Paper 5 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/ecir22.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2112.11294">
                        <papertitle>Extending CLIP for Category-to-Image Retrieval in E-commerce<a href="data/paper_17.pdf">.</a></papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Maurits Bleeker,
                      <a href="https://svakulenk0.github.io/">Svitlana Vakulenko</a>,
                      <a href="https://nanne.github.io/">Nanne van Noord</a>,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>ECIR</em>, 2022
                      <br>
                      <a href="https://arxiv.org/abs/2112.11294">arXiv</a> /
                      <a href="https://github.com/mariyahendriksen/ecir2022_category_to_image_retrieval">Github</a>
                      <p></p>

                      </p>
                    </td>
                  </tr>

                  <!-- Paper 6 -->
                  <tr class="paper-item">
                    <td style="padding:10px;width:30%;vertical-align:middle">
                        <img src="images/sigir-ecom21.png" alt="VennDiagram" width="100%">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2012.08777">
                        <papertitle>Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs. Identified Customers<a href="data/thesis_benedict.pdf">.</a></papertitle>
                      </a>
                      <br>
                      <strong>Mariya Hendriksen</strong>,
                      Ernst Kuiper,
                      Pim Nauts,
                      Sebastian Schelter,
                      <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a>
                      <br>
                      <em>SIGIR eCom</em>, 2020
                      <br>
                      <a href="https://arxiv.org/abs/2012.08777">arXiv</a>
                      <p></p>
                       
                      </p>
                    </td>
                  </tr>

          </tbody></table>

          <!-- Footer -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:center;font-size:small;">
                    Build upon <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's template</a>.
                  </p>
                </td>
              </tr>
            </tbody></table>
          </td>

        </tr>
      </table>

    <!-- Theme Toggle Script -->
    <script>
      // Theme toggle functionality
      (function() {
        const root = document.documentElement;
        const btn = document.getElementById('theme-toggle');
        const saved = localStorage.getItem('theme');
        const systemDark = window.matchMedia('(prefers-color-scheme: dark)').matches;

        // Initialize theme on page load
        if (saved === 'dark' || (!saved && systemDark)) {
          root.classList.add('dark');
          btn.textContent = '☀️';
        }

        // Toggle theme when button is clicked
        btn.addEventListener('click', () => {
          const isDark = root.classList.toggle('dark');
          localStorage.setItem('theme', isDark ? 'dark' : 'light');
          btn.textContent = isDark ? '☀️' : '🌙';
        });

        // Listen for system theme changes
        window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', (e) => {
          if (!localStorage.getItem('theme')) {
            root.classList.toggle('dark', e.matches);
            btn.textContent = e.matches ? '☀️' : '🌙';
          }
        });
      })();
    </script>

    <!-- Animation Reveal Script -->
    <script>
    // Scroll-triggered reveal animations
    document.addEventListener('DOMContentLoaded', function() {
      // Configuration
      const observerOptions = {
        threshold: 0.15,        // Trigger when 15% of element is visible
        rootMargin: '0px 0px -50px 0px'  // Start animation 50px before element enters view
      };

      // Create intersection observer
      const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            // Add reveal class to trigger animation
            entry.target.classList.add('reveal');
            
            // Optional: Stop observing once revealed (for performance)
            observer.unobserve(entry.target);
          }
        });
      }, observerOptions);

      // Select all elements to animate
      const elementsToReveal = document.querySelectorAll('.paper-item, .section-title, .news-item, .profile-content');
      
      // Start observing each element
      elementsToReveal.forEach(element => {
        observer.observe(element);
      });

      // Optional: Reveal profile immediately on page load
      setTimeout(() => {
        const profileElements = document.querySelectorAll('.profile-content');
        profileElements.forEach(el => el.classList.add('reveal'));
      }, 200);
    });

    // Page load animation
    window.addEventListener('load', function() {
      document.body.style.opacity = '1';
      document.body.style.transform = 'translateY(0)';
    });
    </script>

  </body>
</html>